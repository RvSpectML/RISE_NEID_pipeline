#!/bin/bash

####  Job name
#PBS -N snakemake1

####  Request resources here
####    These are typically, number of processors, amount of memory,
####    an the amount of time a job requires.  May include processor
####    type, too.

#PBS -l nodes=1:ppn=1
#PBS -l pmem=16000mb
#PBS -l walltime=24:00:00

#### #### ####  These are the least frequently changing options

####  Your e-mail address and when you want e-mail
#PBS -A open
##PBS -M dus73@psu.edu
#PBS -M ebf11@psu.edu
#PBS -m abe

####  Join output and error; pass environment to job

#PBS -j oe
#PBS -o logs/
#PBS -V
#PBS -W umask=002

##  Print the nodename(s) to the output in case needed for diagnostics,
##  or if you need information about the hardware after the job ran.
if [ -e "$PBS_NODEFILE" ] ; then
    echo "Running on"
    uniq -c $PBS_NODEFILE
fi


##  Change to the directory from which you submit the job, if running
##  from within a job
if [ -d "$PBS_O_WORKDIR" ] ; then
    cd $PBS_O_WORKDIR
fi

#source /gpfs/group/ebf11/default/RISE_NEID/venv_snakemake/bin/activate
source /gpfs/group/ebf11/default/pipeline/shared/venv_snakemake/bin/activate

# Path to julia executable to be used to verify downloads
export PATH=/gpfs/group/ebf11/default/sw/julia-1.6.2/bin:$PATH
# Path to julia depot _for user submitting the job_.  
# Do NOT try to have multiple users share one julia depot!
# Keep commented out if your julia depot is in ~/.julia (good to make this a symlink, so not in home directory)
# export JULIA_DEPOT_PATH=/gpfs/group/ebf11/default/sw/.julia  # For Danying's depot


####################################################################
# Option 1.                                                        #
# initiating snakemake and running workflow in a single submission #
####################################################################

snakemake -c1 --keep-going

####################################################################
# Option 2.                                                        #
# Initiating snakemake and running workflow in cluster mode        #
# update group0 (number of targets grouped together) if necessary. #
####################################################################

#snakemake --keep-going --profile config/pbs-torque/ --latency-wait 20 --groups manifest=group0 ccfs=group0 daily_report=group0 --group-components group0=5 --forcerun manifest

# Printing out job summary
qstat -f $PBS_JOBID
