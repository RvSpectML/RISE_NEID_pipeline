PIPELINE_DIR = config["pipeline_dir"]
INSTRUMENT = config["INSTRUMENT"]
INPUT_VERSION = config["INPUT_VERSION"]
PIPELINE_ID = config["PIPELINE_ID"]
NEID_SOLAR_SCRIPTS = PIPELINE_DIR + "/neid_solar/" + config["NEID_SOLAR_SCRIPTS"]
NEID_NIGHT_SCRIPTS = PIPELINE_DIR + "/neid_night/" + config["NEID_NIGHT_SCRIPTS"]
NEID_SSOF_SCRIPTS = PIPELINE_DIR + "/neid_night/" + config["NEID_SSOF_SCRIPTS"]
DATA_ROOT = f"{PIPELINE_DIR}/{INSTRUMENT}/data"
No_DATA_DAYS_FILENAME = config["No_DATA_DAYS_FILENAME"]
DOWNLOAD_SCRIPT = PIPELINE_DIR + "/" + config["DOWNLOAD_SCRIPT"]
PROGRAMS = config["PROGRAMS"]
SSOF_FLAGS = config["params"]["ssof_flags"]
LAST_ORDER = config["params"]["last_order"]
MAX_SPECTRA_NUMBER = config["params"]["max_spectra_number"]

# parse orders
ORDER_STR = config["params"]["orders"]
ORDERS = []
for x in ORDER_STR.split(","): 
    l = x.strip().split("-")
    if len(l) == 1:
        ORDERS.append(l[0])
    elif len(l) == 2:
        ORDERS.extend(range(int(l[0]), int(l[1]) + 1))
    else:
        raise Exception("Can't parse ORDER_STR!")

CUTOFF_TIME_OF_DAY_START = config["CUTOFF_TIME_OF_DAY_START"]
CUTOFF_TIME_OF_DAY_END = config["CUTOFF_TIME_OF_DAY_END"]

import os, shutil
from datetime import datetime, timedelta
from math import floor
from pip._vendor import tomli
from pyneid.neid import Neid
import pandas as pd

# Use the USER_ID provided in config.yaml if provided;
# otherwise, use the environment variable USER
envvars:
    "USER"
if config["USER_ID"]:
    USER_ID = config["USER_ID"]
else:
    USER_ID = os.environ["USER"]

# Process days between given dates
start_date = datetime.strptime(config["start_date"], "%Y-%m-%d").date()
end_date = datetime.strptime(config["end_date"], "%Y-%m-%d").date()
delta = end_date - start_date
DATES = [(start_date + timedelta(days=x)).strftime("%Y/%m/%d") for x in range(delta.days + 1)]

start_month = start_date.year * 12 + start_date.month
end_month = end_date.year * 12 + end_date.month
YEAR_MONTHS = [f"{floor(x/12)-1}/12" if x%12 == 0 else f"{floor(x/12)}/{x%12:02d}" for x in range(start_month, end_month + 1)]

# Get NExScI login credentials
NEXSCI_ID = config["NEXSCI_ID"]

# Create cookie if it not already exists
COOKIE = "cookie"
if not os.path.exists(COOKIE):
    # parse NEXSCI file for user and password
    with open(NEXSCI_ID, "rb") as f:
        toml_dict = tomli.load(f)
        Neid.login(userid=toml_dict["user"], password=toml_dict["password"], cookiepath=COOKIE)


# Get all objects including the standard stars defined in config and the objects in the programs
def get_objects():
    if config["INCLUDE_STARS_IN_DATA_FOLDER"]:
        # existing object folders
        objects, = glob_wildcards(f"{DATA_ROOT}/{{obj}}/v{INPUT_VERSION}/L2")
    else:
        objects = []
    
    # add the objects explicitly listed in config
    if config["STANDARD_STARS"]:
        objects.extend(config["STANDARD_STARS"]) 
    
    # add objects in programs in config
    programs = config["PROGRAMS"]
    if programs:
        for program in programs:
            param={'datetime': f'{start_date} 00:00:00/{end_date} 23:59:59', 'datalevel': 'l2', 'program': program}
            metafile = f'meta_{program}.csv'
            Neid.query_criteria(param, format='csv', outpath=metafile, cookiepath=COOKIE)
            df = pd.read_csv(metafile)
            objects.extend(pd.unique(df['object']))
    
    objects = pd.unique(objects)
    return objects
    
OBJECTS = get_objects()

# remove the L2 folders with no data downloaded
# and add the date to no_data_days.csv if there's at least one more recent day with data.
def update_no_data_days():
    objects, = glob_wildcards(f"{DATA_ROOT}/{{obj}}/v{INPUT_VERSION}/L2")
    for obj in objects:
        L2_folder = f"{DATA_ROOT}/{obj}/v{INPUT_VERSION}/L2"
        # find dates with no data downloaded
        dates_no_data, = glob_wildcards(f"{L2_folder}/{{date}}/0_no_data_available")
        
        # find date with data
        dates_data, = glob_wildcards(f"{L2_folder}/{{date}}/meta.csv")
        
        if len(dates_data) > 0:
            # get the most recent date with data
            date_data_max = max(dates_data)
            
            # empty dates that are older than available data are to be saved into the csv
            dates_to_csv = [ x for x in dates_no_data if x < date_data_max ]
            
            # try to add dates from existing no_data_days.csv
            try:
                df = pd.read_csv(f'{L2_folder}/{No_DATA_DAYS_FILENAME}')
                dates_already_in_csv = [x for x in df["dates"] if x not in dates_data]
                dates_to_csv.extend(dates_already_in_csv)
            except:
                pass                    
                               
            # sort unique dates and save to file
            df = pd.DataFrame(sorted(set(dates_to_csv)), columns=["dates"])
            df.to_csv(f'{L2_folder}/{No_DATA_DAYS_FILENAME}', index=False)
                    
        # remove folders with dates_no_data
        for date in dates_no_data:
            shutil.rmtree(f"{L2_folder}/{date}")


rule daily_manifest:
    input:
        expand(f"{DATA_ROOT}/{{object}}/v{INPUT_VERSION}/manifest/{{date}}/manifest.csv", object=OBJECTS, date=DATES)


rule ssof_summary:
    input:
        expand(f"{DATA_ROOT}/{{object}}/v{INPUT_VERSION}/outputs/{USER_ID}/{PIPELINE_ID}/ssof/{{ssof_flag_key}}/summary.toml", object=OBJECTS, ssof_flag_key=list(SSOF_FLAGS.keys()))
        
    
rule download_L2:
    output:
        meta=f"{DATA_ROOT}/{{object}}/v{INPUT_VERSION}/L2/{{date}}/meta.csv",
        verified=f"{DATA_ROOT}/{{object}}/v{INPUT_VERSION}/L2/{{date}}/0_download_verified"
    run:
        # for test purpose
        # shell(f"touch '{{output.meta}}'")
        # shell(f"touch '{{output.verified}}'")
        
        # flag whether to run the download code
        flag = True
        
        # skip running the download code iff RUN_ALL_DATES=no and date is in no_data_days.csv
        if config["RUN_ALL_DATES"] == "no":
            try:
                df = pd.read_csv(f"{DATA_ROOT}/{wildcards.object}/v{INPUT_VERSION}/L2/{No_DATA_DAYS_FILENAME}")
                
                if wildcards.date in list(df['dates']):
                    flag = False
            except Exception as e:
                print(str(e))
        
        if flag:
            shell(f"python {DOWNLOAD_SCRIPT} '{DATA_ROOT}/{{wildcards.object}}/v{INPUT_VERSION}/L2' {{wildcards.date}} {INPUT_VERSION} 2 --object '{{wildcards.object}}' --cookie {COOKIE} --cutoff_time_of_day {CUTOFF_TIME_OF_DAY_START} {CUTOFF_TIME_OF_DAY_END}")
            shell(f"[ ! -f '{DATA_ROOT}/{{wildcards.object}}/v{INPUT_VERSION}/L2/{{wildcards.date}}/0_no_data_available' ] && julia --project={NEID_SOLAR_SCRIPTS} {NEID_SOLAR_SCRIPTS}/scripts/verify_download.jl '{DATA_ROOT}/{{wildcards.object}}/v{INPUT_VERSION}/L2/{{wildcards.date}}' --checksums")
            shell(f"if [ -f '{DATA_ROOT}/{{wildcards.object}}/v{INPUT_VERSION}/L2/{{wildcards.date}}/meta_redownload.csv' ]; then python {DOWNLOAD_SCRIPT} '{DATA_ROOT}/{{wildcards.object}}/v{INPUT_VERSION}/L2' {{wildcards.date}} {INPUT_VERSION} 2 --object '{{wildcards.object}}' --cookie {COOKIE} --cutoff_time_of_day {CUTOFF_TIME_OF_DAY_START} {CUTOFF_TIME_OF_DAY_END} && julia --project={NEID_SOLAR_SCRIPTS} {NEID_SOLAR_SCRIPTS}/scripts/verify_download.jl '{DATA_ROOT}/{{wildcards.object}}/v{INPUT_VERSION}/L2/{{wildcards.date}}' --checksums; fi")


rule prep_manifest:
    input:
        f"{DATA_ROOT}/{{object}}/v{INPUT_VERSION}/L2/{{date}}/0_download_verified"
    output:
        f"{DATA_ROOT}/{{object}}/v{INPUT_VERSION}/manifest/{{date}}/manifest.csv"
    version: config["MANIFEST_VERSION"]
    run:
        #shell(f"touch '{{output}}'")
        shell(f"julia --project={NEID_NIGHT_SCRIPTS} {NEID_NIGHT_SCRIPTS}/make_manifest_night_v1.0.jl {DATA_ROOT}/'{{wildcards.object}}'/v{INPUT_VERSION}/L2/{{wildcards.date}} {DATA_ROOT}/'{{wildcards.object}}'/v{INPUT_VERSION}/manifest/{{wildcards.date}} ") 
        
        
rule merge_manifests:
    output:
        f"{DATA_ROOT}/{{object}}/v{INPUT_VERSION}/manifest/manifest.csv"
    run:
        #shell(f"touch '{{output}}'")
        manifest_root = f"{DATA_ROOT}/'{{wildcards.object}}'/v{INPUT_VERSION}/manifest"
        shell(f"[ -f {manifest_root}/manifest.csv ] &&  rm {manifest_root}/manifest.csv'")
        shell(f"julia --project={NEID_SSOF_SCRIPTS} {NEID_SSOF_SCRIPTS}/NEID/merge_manifests.jl {manifest_root} {manifest_root}/manifest.csv")   

        
rule prep_ssof:
    input:
        f"{DATA_ROOT}/{{object}}/v{INPUT_VERSION}/manifest/manifest.csv" 
    output:
        f"{DATA_ROOT}/{{object}}/v{INPUT_VERSION}/outputs/{USER_ID}/{PIPELINE_ID}/ssof/{LAST_ORDER}/data.jld2"
    run:
        #shell(f"touch '{{output}}'")
        shell(f"julia --project={NEID_SSOF_SCRIPTS} {NEID_SSOF_SCRIPTS}/NEID/init.jl {{input}} {DATA_ROOT}/'{{wildcards.object}}'/v{INPUT_VERSION}/outputs/{USER_ID}/{PIPELINE_ID}/ssof {MAX_SPECTRA_NUMBER}")

        
rule analyze_ssof:
    input:
        f"{DATA_ROOT}/{{object}}/v{INPUT_VERSION}/outputs/{USER_ID}/{PIPELINE_ID}/ssof/{LAST_ORDER}/data.jld2"
    output:
        results=f"{DATA_ROOT}/{{object}}/v{INPUT_VERSION}/outputs/{USER_ID}/{PIPELINE_ID}/ssof/{{ssof_flag_key}}/{{order}}/results.jld2",
        results_curv=f"{DATA_ROOT}/{{object}}/v{INPUT_VERSION}/outputs/{USER_ID}/{PIPELINE_ID}/ssof/{{ssof_flag_key}}/{{order}}/results_curv.jld2",
        results_boot=f"{DATA_ROOT}/{{object}}/v{INPUT_VERSION}/outputs/{USER_ID}/{PIPELINE_ID}/ssof/{{ssof_flag_key}}/{{order}}/results_boot.jld2"
    params:
        prefix = f"{DATA_ROOT}/{{object}}/v{INPUT_VERSION}/outputs/{USER_ID}/{PIPELINE_ID}/ssof/{{ssof_flag_key}}/{{order}}"
    run:
        shell(f"touch '{{params.prefix}}/results.jld2'")
        shell(f"touch '{{params.prefix}}/results_curv.jld2'")
        shell(f"touch '{{params.prefix}}/results_boot.jld2'")
        
        
rule post_ssof:
    input:
        lambda wildcards:expand(f"{DATA_ROOT}/{wildcards.object}/v{INPUT_VERSION}/outputs/{USER_ID}/{PIPELINE_ID}/ssof/{wildcards.ssof_flag_key}/{{order}}/results.jld2", order=ORDERS)
    output:
        results_ssof=f"{DATA_ROOT}/{{object}}/v{INPUT_VERSION}/outputs/{USER_ID}/{PIPELINE_ID}/ssof/{{ssof_flag_key}}/results_ssof.csv",
        # TODO: confirm png name
        png=f"{DATA_ROOT}/{{object}}/v{INPUT_VERSION}/outputs/{USER_ID}/{PIPELINE_ID}/ssof/{{ssof_flag_key}}/ssof.png"
    run:
        shell(f"touch '{{output.png}}'")
        shell(f"touch '{{output.results_ssof}}'")
       

rule report_target_ssof:
    input:
        f"{DATA_ROOT}/{{object}}/v{INPUT_VERSION}/outputs/{USER_ID}/{PIPELINE_ID}/ssof/{{ssof_flag_key}}/results_ssof.csv"
    output:
        f"{DATA_ROOT}/{{object}}/v{INPUT_VERSION}/outputs/{USER_ID}/{PIPELINE_ID}/ssof/{{ssof_flag_key}}/summary.toml"
    run:
        shell(f"touch '{{output}}'")


onerror:
    update_no_data_days()
    
onsuccess:
    update_no_data_days()
