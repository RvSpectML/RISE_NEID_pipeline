# retrieve directories from config file
# retrieve directories from config file
PIPELINE_DIR = config["pipeline_dir"]
INSTRUMENT = config["INSTRUMENT"]
INPUT_VERSION = config["INPUT_VERSION"]
PIPELINE_ID = config["PIPELINE_ID"]
NEID_SOLAR_SCRIPTS = PIPELINE_DIR + "/neid_solar/" + config["NEID_SOLAR_SCRIPTS"]
DATA_ROOT = f"{PIPELINE_DIR}/{INSTRUMENT}/data"
No_DATA_DAYS_FILENAME = config["No_DATA_DAYS_FILENAME"]
DOWNLOAD_SCRIPT = PIPELINE_DIR + "/" + config["DOWNLOAD_SCRIPT"]
PROGRAMS = config["PROGRAMS"]
LINELISTS = config["params"]["linelist"]
CCFS_FLAGS = config["params"]["calc_order_ccfs_flags"]
PREP_CCF_DATE_PATH = config["params"]["prep_ccf_date_path"]
PREP_CCF_DATE_STR = PREP_CCF_DATE_PATH.replace("/", "", 2)
CCF_TEMPLATE_DATE = config["params"]["ccf_template_date"]

import os, shutil
from datetime import datetime, timedelta
from math import floor
from pyneid.neid import Neid
import pandas as pd

# Use the USER_ID provided in config.yaml if provided;
# otherwise, use the environment variable USER
envvars:
    "USER"
if config["USER_ID"]:
    USER_ID = config["USER_ID"]
else:
    USER_ID = os.environ["USER"]

# Process days between given dates
start_date = datetime.strptime(config["start_date"], "%Y-%m-%d").date()
end_date = datetime.strptime(config["end_date"], "%Y-%m-%d").date()
delta = end_date - start_date
DATES = [(start_date + timedelta(days=x)).strftime("%Y/%m/%d") for x in range(delta.days + 1)]

start_month = start_date.year * 12 + start_date.month
end_month = end_date.year * 12 + end_date.month
YEAR_MONTHS = [f"{floor(x/12)-1}/12" if x%12 == 0 else f"{floor(x/12)}/{x%12:02d}" for x in range(start_month, end_month + 1)]

# Get NExScI login credentials
NEXSCI_ID = config["NEXSCI_ID"]

# Get cookie
COOKIE = config["COOKIE"] 

# Get all objects including the standard stars defined in config and the objects in the programs
def get_objects():
    if config["INCLUDE_STARS_IN_DATA_FOLDER"]:
        # existing object folders
        objects, = glob_wildcards(f"{DATA_ROOT}/{{obj}}/v{INPUT_VERSION}/L2")
    else:
        objects = []
    
    # add the objects explicitly listed in config
    if config["STANDARD_STARS"]:
        objects.extend(config["STANDARD_STARS"]) 
    
    # add objects in programs in config
    programs = config["PROGRAMS"]
    if programs:
        for program in programs:
            param={'datetime': f'{start_date} 00:00:00/{end_date} 23:59:59', 'datalevel': 'l2', 'program': program}
            metafile = f'meta_{program}.csv'
            Neid.query_criteria(param, format='csv', outpath=metafile, cookiepath=COOKIE)
            df = pd.read_csv(metafile)
            objects.extend(pd.unique(df['object']))
    
    objects = pd.unique(objects)
    return objects


rule all:
    input:
        lambda wildcards: expand(f"{DATA_ROOT}/{{object}}/v{INPUT_VERSION}/outputs/{USER_ID}/{PIPELINE_ID}/{{date}}/daily_summary_{{linelist_key}}_{{ccfs_flag_key}}.toml", object=get_objects(), date=DATES, linelist_key=list(LINELISTS.keys()), ccfs_flag_key=list(CCFS_FLAGS.keys()))
        
    
rule download_L2:
    output:
        meta=f"{DATA_ROOT}/{{object}}/v{INPUT_VERSION}/L2/{{date}}/meta.csv",
        verified=f"{DATA_ROOT}/{{object}}/v{INPUT_VERSION}/L2/{{date}}/0_download_verified"
    run:
        # flag whether to run the download code
        flag = True
        
        # skip running the download code iff run_all_dates=no and date is in no_data_days.csv
        if config["run_all_dates"] == "no":
            try:
                df = pd.read_csv(f"{DATA_ROOT}/{wildcards.object}/v{INPUT_VERSION}/L2/{No_DATA_DAYS_FILENAME}")
                
                if wildcards.date in list(df['dates']):
                    flag = False
            except Exception as e:
                print(str(e))
        
        if flag:
            shell(f"python {DOWNLOAD_SCRIPT} '{DATA_ROOT}/{{wildcards.object}}/v{INPUT_VERSION}/L2' {{wildcards.date}} {INPUT_VERSION} 2 --object '{{wildcards.object}}' --cookie {COOKIE}")
            shell(f"[ ! -f '{DATA_ROOT}/{{wildcards.object}}/v{INPUT_VERSION}/L2/{{wildcards.date}}/0_no_data_available' ] && julia --project={NEID_SOLAR_SCRIPTS} {NEID_SOLAR_SCRIPTS}/scripts/verify_download.jl '{DATA_ROOT}/{{wildcards.object}}/v{INPUT_VERSION}/L2/{{wildcards.date}}' --checksums")
            shell(f"if [ -f '{DATA_ROOT}/{{wildcards.object}}/v{INPUT_VERSION}/L2/{{wildcards.date}}/meta_redownload.csv' ]; then python {DOWNLOAD_SCRIPT} '{DATA_ROOT}/{{wildcards.object}}/v{INPUT_VERSION}/L2' {{wildcards.date}} {INPUT_VERSION} 2  && julia --project={NEID_SOLAR_SCRIPTS} {NEID_SOLAR_SCRIPTS}/scripts/verify_download.jl '{DATA_ROOT}/{{wildcards.object}}/v{INPUT_VERSION}/L2/{{wildcards.date}}' --checksums; fi")


rule prep_manifest:
    input:
        f"{DATA_ROOT}/{{object}}/v{INPUT_VERSION}/L2/{{date}}/0_download_verified"
    output:
        f"{DATA_ROOT}/{{object}}/v{INPUT_VERSION}/manifest/{{date}}/manifest.csv"
    version: config["MANIFEST_VERSION"]
    run:
        shell("touch '{output}'")


rule calc_ccfs:
    input:
        manifest=f"{DATA_ROOT}/{{object}}/v{INPUT_VERSION}/manifest/{{date}}/manifest.csv",
        #linelist_file=lambda wildcards:f"{DATA_ROOT}/{{object}}/v{INPUT_VERSION}/outputs/{USER_ID}/{PIPELINE_ID}/linelist_{PREP_CCF_DATE_STR}_{{linelist_key}}_{{ccfs_flag_key}}.csv",
        #anchors=f"{DATA_ROOT}/{{object}}/v{INPUT_VERSION}/outputs/{USER_ID}/{PIPELINE_ID}/anchors_{PREP_CCF_DATE_STR}_{{linelist_key}}_{{ccfs_flag_key}}.jld2" 
    output:
        f"{DATA_ROOT}/{{object}}/v{INPUT_VERSION}/outputs/{USER_ID}/{PIPELINE_ID}/{{date}}/daily_ccfs_{{linelist_key}}_{{ccfs_flag_key}}.jld2"
    version: config["CCFS_VERSION"]
    params:
        orders_first=config["params"]["orders_first"],
        orders_last=config["params"]["orders_last"],
        range_no_mask_change=config["params"]["range_no_mask_change"],
        ccfs_flag_value=lambda wildcards:CCFS_FLAGS[wildcards.ccfs_flag_key]
    run:
        shell("touch '{output}'")
    
rule calc_rvs:
    input:
        ccfs=f"{DATA_ROOT}/{{object}}/v{INPUT_VERSION}/outputs/{USER_ID}/{PIPELINE_ID}/{{date}}/daily_ccfs_{{linelist_key}}_{{ccfs_flag_key}}.jld2",
        #template=f"{DATA_ROOT}/{{object}}/v{INPUT_VERSION}/outputs/{USER_ID}/{PIPELINE_ID}/{CCF_TEMPLATE_DATE}/daily_ccfs_{{linelist_key}}_{{ccfs_flag_key}}.jld2"
    output:
        f"{DATA_ROOT}/{{object}}/v{INPUT_VERSION}/outputs/{USER_ID}/{PIPELINE_ID}/{{date}}/daily_rvs_{{linelist_key}}_{{ccfs_flag_key}}.csv"
    version: config["RVS_VERSION"]
    params:
        daily_rvs_flags=config["params"]["daily_rvs_flags"]
    run:
        shell("touch '{output}'")


rule report_daily:
    input:
        rvs=f"{DATA_ROOT}/{{object}}/v{INPUT_VERSION}/outputs/{USER_ID}/{PIPELINE_ID}/{{date}}/daily_rvs_{{linelist_key}}_{{ccfs_flag_key}}.csv"
    output:
        f"{DATA_ROOT}/{{object}}/v{INPUT_VERSION}/outputs/{USER_ID}/{PIPELINE_ID}/{{date}}/daily_summary_{{linelist_key}}_{{ccfs_flag_key}}.toml"
    version: config["REPORT_DAILY_VERSION"]
    run:
        shell("touch '{output}'")

rule report_monthly:
    input:
        #f"{DATA_ROOT}/{{object}}/v{INPUT_VERSION}/outputs/{USER_ID}/{PIPELINE_ID}/{{date}}/daily_summary_{{linelist_key}}_{{ccfs_flag_key}}.toml"
    output:
        good=f"{DATA_ROOT}/{{object}}/v{INPUT_VERSION}/outputs/{USER_ID}/{PIPELINE_ID}/{{year_month}}/monthly_summary_{{linelist_key}}_{{ccfs_flag_key}}.csv",
        bad=f"{DATA_ROOT}/{{object}}/v{INPUT_VERSION}/outputs/{USER_ID}/{PIPELINE_ID}/{{year_month}}/monthly_summary_incl_bad_{{linelist_key}}_{{ccfs_flag_key}}.csv"
    version: config["REPORT_MONTHLY_VERSION"]
    run:
        shell("touch '{output.good}'")
        shell("touch '{output.bad}'")

rule report_all:
    input:
        #daily_summary = f"{DATA_ROOT}/{{object}}/v{INPUT_VERSION}/outputs/{USER_ID}/{PIPELINE_ID}/{{date}}/daily_summary_{{linelist_key}}_{{ccfs_flag_key}}.toml"
    output:
        good=f"{DATA_ROOT}/{{object}}/v{INPUT_VERSION}/outputs/{USER_ID}/{PIPELINE_ID}/summary_{{linelist_key}}_{{ccfs_flag_key}}.csv",
        bad=f"{DATA_ROOT}/{{object}}/v{INPUT_VERSION}/outputs/{USER_ID}/{PIPELINE_ID}/summary_incl_bad_{{linelist_key}}_{{ccfs_flag_key}}.csv"
    version: config["REPORT_ALL_VERSION"]
    run:
        shell("touch '{output.good}'")
        shell("touch '{output.bad}'")


rule combine_rvs:
    output:
        good=f"{DATA_ROOT}/{{object}}/v{INPUT_VERSION}/outputs/{USER_ID}/{PIPELINE_ID}/combined_rvs_{{linelist_key}}_{{ccfs_flag_key}}.csv",
        bad=f"{DATA_ROOT}/{{object}}/v{INPUT_VERSION}/outputs/{USER_ID}/{PIPELINE_ID}/combined_rvs_incl_bad_{{linelist_key}}_{{ccfs_flag_key}}.csv"
    version: config["COMBINE_RVS_VERSION"]
    run:
        shell("touch '{output.good}'")
        shell("touch '{output.bad}'")

onerror:
    # remove the L2 folders with no data downloaded
    # and add the date to no_data_days.csv if there's at least one more recent day with data.
    objects, = glob_wildcards(f"{DATA_ROOT}/{{obj}}/v{INPUT_VERSION}/L2")
    for obj in objects:
        L2_folder = f"{DATA_ROOT}/{obj}/v{INPUT_VERSION}/L2"
        # find dates with no data downloaded
        dates_no_data, = glob_wildcards(f"{L2_folder}/{{date}}/0_no_data_available")
        
        # find date with data
        dates_data, = glob_wildcards(f"{L2_folder}/{{date}}/meta.csv")
        
        if len(dates_data) > 0:
            # get the most recent date with data
            date_data_max = max(dates_data)
            
            # empty dates that are older than available data are to be saved into the csv
            dates_to_csv = [ x for x in dates_no_data if x < date_data_max ]
            
            if len(dates_to_csv) > 0:
                # try to add dates from existing no_data_days.csv
                try:
                    df = pd.read_csv(f'{L2_folder}/{No_DATA_DAYS_FILENAME}')
                    dates_to_csv.extend(list(df["dates"]))
                except:
                    pass
                    
                # sort unique dates and save to file
                df = pd.DataFrame(sorted(set(dates_to_csv)), columns=["dates"])
                df.to_csv(f'{L2_folder}/{No_DATA_DAYS_FILENAME}', index=False)
                    
        # remove folders with dates_no_data
        for date in dates_no_data:
            shutil.rmtree(f"{L2_folder}/{date}")

