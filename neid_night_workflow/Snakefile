PIPELINE_DIR = config["pipeline_dir"]
INSTRUMENT = config["INSTRUMENT"]
INPUT_VERSION = config["INPUT_VERSION"]
PIPELINE_ID = config["PIPELINE_ID"]
NEID_SOLAR_SCRIPTS = PIPELINE_DIR + "/neid_solar/" + config["NEID_SOLAR_SCRIPTS"]
NEID_NIGHT_SCRIPTS = PIPELINE_DIR + "/neid_night/" + config["NEID_NIGHT_SCRIPTS"]
NEID_SSOF_SCRIPTS = PIPELINE_DIR + "/neid_night/" + config["NEID_SSOF_SCRIPTS"]
DATA_ROOT = f"{PIPELINE_DIR}/{INSTRUMENT}/data"
DOWNLOAD_SCRIPT = PIPELINE_DIR + "/" + config["DOWNLOAD_SCRIPT"]
PROGRAMS = config["PROGRAMS"]
SSOF_FLAGS = config["params"]["ssof_flags"]
LAST_ORDER = config["params"]["last_order"]
MAX_SPECTRA_NUMBER = config["params"]["max_spectra_number"]
OBJECT_DATES_FILE = config["OBJECT_DATES_FILE"]

# parse orders
ORDER_STR = config["params"]["orders"]
ORDERS = []
for x in ORDER_STR.split(","): 
    l = x.strip().split("-")
    if len(l) == 1:
        ORDERS.append(l[0])
    elif len(l) == 2:
        ORDERS.extend(range(int(l[0]), int(l[1]) + 1))
    else:
        raise Exception("Can't parse ORDER_STR!")

CUTOFF_TIME_OF_DAY_START = config["CUTOFF_TIME_OF_DAY_START"]
CUTOFF_TIME_OF_DAY_END = config["CUTOFF_TIME_OF_DAY_END"]

import json
import os
import pandas as pd
from datetime import datetime, timedelta
from math import floor
from pip._vendor import tomli
from pyneid.neid import Neid


# Use the USER_ID provided in config.yaml if provided;
# otherwise, use the environment variable USER
envvars:
    "USER"
if config["USER_ID"]:
    USER_ID = config["USER_ID"]
else:
    USER_ID = os.environ["USER"]
    
# get start/end datetime
start_datetime = f"{config['start_date']} {CUTOFF_TIME_OF_DAY_START}"
end_datetime = f"{config['end_date']} {CUTOFF_TIME_OF_DAY_END}"

# Get NExScI login credentials
NEXSCI_ID = config["NEXSCI_ID"]

# Create cookie if it not already exists
COOKIE = "cookie"
if not os.path.exists(COOKIE):
    # parse NEXSCI file for user and password
    with open(NEXSCI_ID, "rb") as f:
        toml_dict = tomli.load(f)
        Neid.login(userid=toml_dict["user"], password=toml_dict["password"], cookiepath=COOKIE)

# convert a datetime in the format "%Y-%m-%d %H:%M:%S.%f" to the date bucket it belongs to in the format "%Y/%m%d"
def meta_datetime_to_date(s):
    shift_time = datetime.strptime(CUTOFF_TIME_OF_DAY_END, "%H:%M:%S")
    shift_hours = shift_time.hour + shift_time.minute / 60
    return (datetime.strptime(s, "%Y-%m-%d %H:%M:%S.%f") - timedelta(hours = shift_hours)).date().strftime("%Y/%m/%d")
   
# Get all objects including the standard stars defined in config and the objects in the programs
def get_object_dates():
    object_dates = {}
    
    # add objects in programs in config
    programs = config["PROGRAMS"]
    if programs:
        for program in programs:
            param={'datetime': f'{start_datetime}/{end_datetime}', 'datalevel': 'l2', 'program': program}
            metafile = f'meta_{program}.csv'
            Neid.query_criteria(param, format='csv', outpath=metafile, cookiepath=COOKIE)
            df = pd.read_csv(metafile)
            df = df[df['swversion'].str.startswith('v' + str(INPUT_VERSION))]  
            # calcuate the date
            df['date'] = df['date_obs'].apply(lambda x: meta_datetime_to_date(x))
            # group by object and date
            for object in pd.unique(df['object']):
                df_object = df[df['object'] == object]
                object_dates[object] = list(pd.unique(df_object['date']))
                for date in object_dates[object]:
                    dir = f"{DATA_ROOT}/{object}/v{INPUT_VERSION}/L2/{date}"
                    if config["RUN_ALL_DATES"] == "yes" or not os.path.exists(f"{dir}/0_download_verified"):
                        df_object_date = df_object[df_object['date'] == date]
                        # save as meta_redownload.csv to the corresponding folder
                        if not os.path.exists(dir):
                            os.makedirs(dir)
                        df_object_date.to_csv(f"{dir}/meta_redownload.csv", index=False) 
                        df_object_date.to_csv(f"{dir}/meta.csv", index=False) 
                    
    # existing object folders
    if config["INCLUDE_STARS_IN_DATA_FOLDER"]:       
        objects, = glob_wildcards(f"{DATA_ROOT}/{{obj}}/v{INPUT_VERSION}/L2")
    else:
        objects = []
    
    # add the objects explicitly listed in config
    if config["STANDARD_STARS"]:
        objects.extend(config["STANDARD_STARS"]) 
        
    for object in objects:
        if object not in object_dates:
            param={'datetime': f'{start_datetime}/{end_datetime}', 'datalevel': 'l2', 'object': object}
            metafile = f'meta_{object}.csv'
            Neid.query_criteria(param, format='csv', outpath=metafile, cookiepath=COOKIE)
            df_object = pd.read_csv(metafile)
            df_object = df_object[df_object['swversion'].str.startswith('v' + str(INPUT_VERSION))] 
            # calcuate the date
            df_object['date'] = df_object['date_obs'].apply(lambda x: meta_datetime_to_date(x))
            object_dates[object] = list(pd.unique(df_object['date']))
            for date in object_dates[object]:
                dir = f"{DATA_ROOT}/{object}/v{INPUT_VERSION}/L2/{date}"
                if config["RUN_ALL_DATES"] == "yes" or not os.path.exists(f"{dir}/0_download_verified"):
                    df_object_date = df_object[df_object['date'] == date]
                    # save as meta_redownload.csv to the corresponding folder
                    if not os.path.exists(dir):
                        os.makedirs(dir)
                    df_object_date.to_csv(f"{dir}/meta.csv", index=False) 
                    df_object_date.to_csv(f"{dir}/meta_redownload.csv", index=False) 
    
    with open(OBJECT_DATES_FILE, "w") as f:
        json.dump(object_dates, f)
        
    return object_dates
     
if os.path.exists(OBJECT_DATES_FILE):
    with open(OBJECT_DATES_FILE, "r") as f:
        OBJECT_DATES = json.load(f)
else:
    OBJECT_DATES = get_object_dates()

rule all:
    input:
        expand(f"{DATA_ROOT}/{{object}}/v{INPUT_VERSION}/outputs/{USER_ID}/{PIPELINE_ID}/ssof/{{ssof_flag_key}}/summary.toml", object=list(OBJECT_DATES.keys()), ssof_flag_key=list(SSOF_FLAGS.keys()))
        
    
rule download_L2:
    output:
        verified=f"{DATA_ROOT}/{{object}}/v{INPUT_VERSION}/L2/{{date}}/0_download_verified"
    run:
        # for test purpose
        #shell(f"touch '{{output.meta}}'")
        #shell(f"touch '{{output.verified}}'")
        
        shell(f"python {DOWNLOAD_SCRIPT} '{DATA_ROOT}/{{wildcards.object}}/v{INPUT_VERSION}/L2' {{wildcards.date}} {INPUT_VERSION} 2 --object '{{wildcards.object}}' --cookie {COOKIE} --cutoff_time_of_day {CUTOFF_TIME_OF_DAY_START} {CUTOFF_TIME_OF_DAY_END}")
        shell(f"julia --project={NEID_SOLAR_SCRIPTS} {NEID_SOLAR_SCRIPTS}/scripts/verify_download.jl '{DATA_ROOT}/{{wildcards.object}}/v{INPUT_VERSION}/L2/{{wildcards.date}}' --checksums")
        shell(f"if [ -f '{DATA_ROOT}/{{wildcards.object}}/v{INPUT_VERSION}/L2/{{wildcards.date}}/meta_redownload.csv' ]; then python {DOWNLOAD_SCRIPT} '{DATA_ROOT}/{{wildcards.object}}/v{INPUT_VERSION}/L2' {{wildcards.date}} {INPUT_VERSION} 2 --object '{{wildcards.object}}' --cookie {COOKIE} --cutoff_time_of_day {CUTOFF_TIME_OF_DAY_START} {CUTOFF_TIME_OF_DAY_END} && julia --project={NEID_SOLAR_SCRIPTS} {NEID_SOLAR_SCRIPTS}/scripts/verify_download.jl '{DATA_ROOT}/{{wildcards.object}}/v{INPUT_VERSION}/L2/{{wildcards.date}}' --checksums; fi")


rule prep_manifest:
    input:
        f"{DATA_ROOT}/{{object}}/v{INPUT_VERSION}/L2/{{date}}/0_download_verified"
    output:
        f"{DATA_ROOT}/{{object}}/v{INPUT_VERSION}/manifest/{{date}}/manifest.csv"
    version: config["MANIFEST_VERSION"]
    run:
        #shell(f"touch '{{output}}'")
        shell(f"julia --project={NEID_NIGHT_SCRIPTS} {NEID_NIGHT_SCRIPTS}/make_manifest_night_v1.0.jl {DATA_ROOT}/'{{wildcards.object}}'/v{INPUT_VERSION}/L2/{{wildcards.date}} {DATA_ROOT}/'{{wildcards.object}}'/v{INPUT_VERSION}/manifest/{{wildcards.date}} ") 
        
        
rule merge_manifests:
    input:
        lambda wildcards:expand(f"{DATA_ROOT}/{wildcards.object}/v{INPUT_VERSION}/manifest/{{date}}/manifest.csv", date=OBJECT_DATES[wildcards.object])
    output:
        f"{DATA_ROOT}/{{object}}/v{INPUT_VERSION}/manifest/manifest.csv"
    run:
        #shell(f"touch '{{output}}'")
        manifest_root = f"{DATA_ROOT}/'{{wildcards.object}}'/v{INPUT_VERSION}/manifest"
        shell(f"if [ -f {manifest_root}/manifest.csv ]; then rm {manifest_root}/manifest.csv; fi")
        shell(f"julia --project={NEID_SSOF_SCRIPTS} {NEID_SSOF_SCRIPTS}/NEID/merge_manifests.jl {manifest_root} {manifest_root}/manifest.csv")   

        
rule prep_ssof:
    input:
        f"{DATA_ROOT}/{{object}}/v{INPUT_VERSION}/manifest/manifest.csv" 
    output:
        f"{DATA_ROOT}/{{object}}/v{INPUT_VERSION}/outputs/{USER_ID}/{PIPELINE_ID}/ssof/{LAST_ORDER}/data.jld2"
    run:
        #shell(f"touch '{{output}}'")
        shell(f"julia --project={NEID_SSOF_SCRIPTS}/NEID {NEID_SSOF_SCRIPTS}/NEID/init.jl {{input}} {DATA_ROOT}/'{{wildcards.object}}'/v{INPUT_VERSION}/outputs/{USER_ID}/{PIPELINE_ID}/ssof {MAX_SPECTRA_NUMBER}")

        
rule analyze_ssof:
    input:
        f"{DATA_ROOT}/{{object}}/v{INPUT_VERSION}/outputs/{USER_ID}/{PIPELINE_ID}/ssof/{LAST_ORDER}/data.jld2"
    output:
        results=f"{DATA_ROOT}/{{object}}/v{INPUT_VERSION}/outputs/{USER_ID}/{PIPELINE_ID}/ssof/{{ssof_flag_key}}/{{order}}/results.jld2",
        results_curv=f"{DATA_ROOT}/{{object}}/v{INPUT_VERSION}/outputs/{USER_ID}/{PIPELINE_ID}/ssof/{{ssof_flag_key}}/{{order}}/results_curv.jld2",
        results_boot=f"{DATA_ROOT}/{{object}}/v{INPUT_VERSION}/outputs/{USER_ID}/{PIPELINE_ID}/ssof/{{ssof_flag_key}}/{{order}}/results_boot.jld2"
    params:
        ssof_flag_value=lambda wildcards:config["params"]["ssof_flags"][wildcards.ssof_flag_key]
    run:
        #shell(f"touch '{{params.prefix}}/results.jld2'")
        #shell(f"touch '{{params.prefix}}/results_curv.jld2'")
        #shell(f"touch '{{params.prefix}}/results_boot.jld2'")
        shell(f"julia --project={NEID_SSOF_SCRIPTS}/NEID {NEID_SSOF_SCRIPTS}/NEID/analysis.jl {DATA_ROOT}/'{{wildcards.object}}'/v{INPUT_VERSION}/outputs/{USER_ID}/{PIPELINE_ID}/ssof/{{wildcards.order}}/data.jld2 {DATA_ROOT}/'{{wildcards.object}}'/v{INPUT_VERSION}/outputs/{USER_ID}/{PIPELINE_ID}/ssof/neid_pipeline.jld2 {{wildcards.order}} {DATA_ROOT}/'{{wildcards.object}}'/v{INPUT_VERSION}/outputs/{USER_ID}/{PIPELINE_ID}/ssof/{{wildcards.ssof_flag_key}}/{{wildcards.order}}/")
        
        
rule post_ssof:
    input:
        lambda wildcards:expand(f"{DATA_ROOT}/{wildcards.object}/v{INPUT_VERSION}/outputs/{USER_ID}/{PIPELINE_ID}/ssof/{wildcards.ssof_flag_key}/{{order}}/results.jld2", order=ORDERS)
    output:
        results_ssof=f"{DATA_ROOT}/{{object}}/v{INPUT_VERSION}/outputs/{USER_ID}/{PIPELINE_ID}/ssof/{{ssof_flag_key}}/results_ssof.csv",
        # TODO: confirm png name
        png=f"{DATA_ROOT}/{{object}}/v{INPUT_VERSION}/outputs/{USER_ID}/{PIPELINE_ID}/ssof/{{ssof_flag_key}}/ssof.png"
    run:
        shell(f"touch '{{output.png}}'")
        shell(f"touch '{{output.results_ssof}}'")
       

rule report_target_ssof:
    input:
        f"{DATA_ROOT}/{{object}}/v{INPUT_VERSION}/outputs/{USER_ID}/{PIPELINE_ID}/ssof/{{ssof_flag_key}}/results_ssof.csv"
    output:
        f"{DATA_ROOT}/{{object}}/v{INPUT_VERSION}/outputs/{USER_ID}/{PIPELINE_ID}/ssof/{{ssof_flag_key}}/summary.toml"
    run:
        shell(f"touch '{{output}}'")
